{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Washington House Price Prediction  -- Capstone Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Content\n",
    "\n",
    "###    1. Introduction\n",
    "###    2. Data Exploratory Analysis\n",
    "###    3. Data Processing\n",
    "###    4. Model Building\n",
    "###    5. Model Comparision\n",
    "###    6. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "## Problem Statement\n",
    "As a house value is simply more than location and square footage. Like the features that make up a person, an educated party would want to know all aspects that give a house its value. For example, if we want to sell a house and we don't know the price which we can take, as it can't be too low or too high. To find house price we usually try to find similar properties in our neighbourhood and based on collected data we trying to assess our house price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Dictionary\n",
    "\n",
    "    1. cid: a notation for a house (house id)\n",
    "    2. dayhours: Date house was sold (date-month-year of sale)\n",
    "    3. price: Price is prediction target (Target variable)\n",
    "    4. room_bed: Number of Bedrooms/House (number of bedrooms per house)\n",
    "    5. room_bath: Number of bathrooms/bedrooms (number of bathrooms per bedrooms)\n",
    "    6. living_measure: square footage of the home\n",
    "    7. lot_measure: square footage of the lot\n",
    "    8. ceil: Total floors (levels) in house (how many floors, ground, 1st floor, 2nd floor ..)\n",
    "    9. coast: House which has a view to a waterfront (house with or without coastview/seafacing)\n",
    "    10. sight: Has been viewed (sight has been viewed before buying this house)\n",
    "    11. condition: How good the condition is (Overall)\n",
    "    12. quality: grade given to the housing unit, based on grading system\n",
    "    13. ceil_measure: square footage of house apart from basement\n",
    "    14. basement_measure: square footage of the basement\n",
    "    15. yr_built: Built Year (year the house was built)\n",
    "    16. yr_renovated: Year when house was renovated (year the house was renovated)\n",
    "    17. zipcode: zip code of the area \n",
    "    18. lat: Latitude coordinate\n",
    "    19. long: Longitude coordinate\n",
    "    20. living_measure15: Living room area in 2015(implies-- some renovations) This might or might not have affected the lotsize area\n",
    "    21. lot_measure15: lotSize area in 2015(implies-- some renovations)\n",
    "    22. furnished: Based on the quality of room\n",
    "    23. total_area: Measure of both living and lot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Initial Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "color = sns.color_palette()\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import important libraries for model building\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy import stats\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read dataset\n",
    "dataset_raw = pd.read_excel(\"innercity.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display head of the complete dataset (with all columns)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "dataset_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('These are the variable columns in the dataset: \\n\\n', dataset_raw.columns )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of columns and rows\n",
    "print('The number of rows (observations) is',dataset_raw.shape[0],'\\n''The number of columns (variables) is',dataset_raw.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_raw.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: \n",
    "\n",
    "1. There are 23 variables. There are 12 float datatypes, 4 integer datatypes and 7 object datatypes.\n",
    "2. There are few time-data columns like dayhours (day at which house was sold), \n",
    "3. yr_built (year on which house was built), and yr_renovated (year on which house was renovated). These variables are not in time format, which we have to change, \"if required\".\n",
    "4. Object datatypes can also be changed to float for seeing their correlation with other variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## statistical summary of the raw dataset\n",
    "dataset_raw.describe().round(2).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with location data\n",
    "\n",
    "#### Using zipcodes to track city, county, state and population of the area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_zipcodes = pd.read_excel(\"USA_Zipcodes.xlsx\")\n",
    "us_zipcodes.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_new = pd.merge(dataset_raw,us_zipcodes, how='left',left_on=['zipcode'],right_on=['zip']).dropna()\n",
    "dataset = dataset_new\n",
    "dataset.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_unique = ('city', 'state_id', 'state_name', 'county_name', 'county_names_all')\n",
    "for i in cols_unique:\n",
    "    print('unique items in', i , ': \\n', dataset[i].unique(), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets drop the unnecessary columns from us zipcodes \n",
    "# and also drop repeated columns like lat and long and rename them as latitude and lontitude\n",
    "\n",
    "dataset.drop(['lat_y', 'lng', 'state_id', 'state_name', 'county_name', 'county_names_all', 'zip'], axis=1, inplace=True)\n",
    "dataset.rename(columns={'lat_x':'latitude', 'long':'longitude'}, inplace=True)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert datatypes for ease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## converting dayhours into timestamp\n",
    "dataset['dayhours'] = pd.to_datetime(dataset['dayhours'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## changing datatype of cid (house id) from int to object\n",
    "dataset['cid']=dataset['cid'].astype(np.object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## changing datatype of zipcode from int to integer datatype\n",
    "dataset = dataset.astype({'zipcode':'int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## printing total number of unique house in the dataset\n",
    "print('The list of unique house ids present in the dataset is',dataset['cid'].nunique(), ', \\n as compared to total number of 21387 rows of the dataset.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that there are many house that are sold more than one times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## list of columns which are float types\n",
    "floatColumns = dataset.dtypes[dataset.dtypes == np.object]\n",
    "print(floatColumns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here, total area and lontitude needs to be converted into numeric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## printing unique values of columns which is object datatype but has numeric value'\n",
    "## we are ignoring total area which is obviously wrongly placed as object\n",
    "\n",
    "object_unique = ('ceil', 'coast', 'condition')\n",
    "for i in object_unique:\n",
    "    print('unique items in', i , ': \\n', dataset[i].unique(), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## list of columns which are numeric types\n",
    "numericColumns = dataset.dtypes[dataset.dtypes != np.object]\n",
    "print(numericColumns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## printing unique values of columns which is float datatype but might have a object type feel'\n",
    "\n",
    "## zipcode is an obvious object which is recorded as float\n",
    "numeric_float_unique = ('room_bed', 'room_bath', 'sight', 'quality' , 'yr_renovated' , 'furnished')\n",
    "for i in numeric_float_unique:\n",
    "    print('unique items in', i , ': \\n', dataset[i].unique(), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Changing datatype from object to float with error coarse to change unwanted strings into NaN values\n",
    "\n",
    "change_datatype_cols = dataset[['ceil','coast','condition', 'yr_built' , 'longitude', 'total_area']]\n",
    "\n",
    "for i in change_datatype_cols:\n",
    "    dataset[i] = pd.to_numeric(dataset[i],errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Converting other date variables from object/int datatype to date datatype\n",
    "    # dataset['yr_built'] = pd.to_datetime(dataset['yr_built'], errors='coerce', format='%Y')\n",
    "    # dataset['yr_renovated'] = pd.to_datetime(dataset['yr_renovated'], format='%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove unwanted string characters\n",
    "   #dataset = dataset.replace(\"$\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## checking datatypes after conversion\n",
    "dataset.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming columns for ease to understand\n",
    "\n",
    "we are renaming the confusing column names for an ease of understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating reference dataframe of the list of columns which are renamed\n",
    "\n",
    "cols_renaming = {'original_Column':['cid','dayhours','room_bed','room_bath','ceil', 'coast', 'sight', 'quality','living_measure','lot_measure','ceil_measure','basement','lat', 'long', 'living_measure15', 'lot_measure15', 'density'],\n",
    "        'renamed_Column':['house_id','date','bedroom','ratio_bathroom','total_floors', 'seaface', 'sight_viewed', 'quality_grade','living_area','lot_area','floor_area','basement_area','latitude', 'longitude', 'living_area_2015', 'lot_area_2015', 'population_density']}\n",
    "  \n",
    "# Create DataFrame\n",
    "df_renamedCols = pd.DataFrame(cols_renaming)\n",
    "df_renamedCols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## renaming all the confusing column names\n",
    "dataset.rename(columns={'cid':'house_id','dayhours':'date','room_bed':'bedroom','room_bath':'ratio_bathroom','ceil':'total_floors', 'coast':'seaface', 'sight':'sight_viewed', 'quality':'quality_grade','living_measure':'living_area','lot_measure':'lot_area','ceil_measure':'floor_area','basement':'basement_area','lat':'latitude', 'long':'longitude', 'living_measure15':'living_area_2015', 'lot_measure15':'lot_area_2015', 'density':'population_density'}, inplace=True)\n",
    "\n",
    "## rechecking head after renaming the columns\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sorting dataset based on date\n",
    "dataset = dataset.sort_values('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## resorted datset\n",
    "dataset.reset_index(inplace=True)\n",
    "dataset.drop(['index'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## checking duplicate rows\n",
    "dataset.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## checking if there are duplicate houses that sold more than one times\n",
    "dataset.duplicated(subset=['house_id']).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: This shows that few of the house are sold multiples times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## house sold multiple times\n",
    "id_count = dataset['house_id'].value_counts()\n",
    "id_count[id_count>1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deriving new columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Adding a new column 'prev_sold'\n",
    "Note: new dataset showing house resold no of times\n",
    "1. 0 mean not resold or sold the very first time. House sold before 0 times. \n",
    "2. 1 means house sold once before current purchase. House sold before 1 times.\n",
    "3. 2 means house sold twice before current purchase. House sold before 2 times.\n",
    "\n",
    "The reason behind doing this is because the price might increase or decrease if a property is resold multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## lets generate a new column that counts in order that how many times a house was sold\n",
    "dataset['prev_sold'] = dataset.groupby('house_id').cumcount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## new dataset showing house resold no of times\n",
    "dataset.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[(dataset['prev_sold'] == 2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. from ratio of bathroom per bedroom to number of bathrooms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['bathroom']=dataset['ratio_bathroom']*dataset['bedroom']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## new dataset showing new bathroom column\n",
    "dataset.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. from Counting house built-years and renovation-years\n",
    "\n",
    "We are counting years of house built and renovation years instead of keeping them as a year number. This will help us in further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## deriving house age from the year house is built by taking next year (2016) as refernce year.\n",
    "## we haven't taken 2015 as ref year orlese we have got 0 for many house built in 2015.\n",
    "\n",
    "dataset['house_age'] = 2016 - dataset['yr_built']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## deriving renovation age\n",
    "\n",
    "dataset['renovation_yrs'] = np.where(dataset['yr_renovated']!= 0, 2016 - dataset['yr_renovated'], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## deriving renovation status\n",
    "dataset['renovated_orNot'] = np.where(dataset['renovation_yrs']!= 0, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Month house was sold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## converting dayhours into timestamp\n",
    "\n",
    "dataset['sold_month'] = pd.to_datetime(dataset['date'], format='%Y-%m')\n",
    "dataset['sold_month'] = dataset['sold_month'].apply(lambda x: x.strftime('%m'))\n",
    "dataset['sold_month'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## changing datatype of zipcode from int to integer datatype\n",
    "dataset = dataset.astype({'sold_month':'int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. deal with areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "areas=dataset[['living_area','lot_area', 'floor_area', 'basement_area', 'total_area', 'living_area_2015', 'lot_area_2015', 'renovated_orNot']]\n",
    "areas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note:\n",
    "Here, we can observe that:\n",
    "1. total_area = living_area + lot_area\n",
    "2. living area = floor_area + basement_area\n",
    "3. even though house is not renovated but still living area and lot area in 2015 has been updated with changed values.\n",
    "4. here we can see that few of the house may not have basement. So we can derive a new columns as basement_orNot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. column mentioning Basement availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"basement_orNot\"]=dataset[\"basement_area\"].apply( lambda x:1 if x>0 else 0)\n",
    "dataset.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminating unnecessary columns\n",
    "\n",
    "Note: We are removing unnecessary columns like house id, yr_built and yr_renovated.\n",
    "We have already extracted meaningful values and created new columns based on these columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.drop(['house_id', 'ratio_bathroom', 'yr_built', 'yr_renovated'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.drop(['date'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistical Summary of the new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#statistical summary\n",
    "dataset.describe().round(2).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking missing values in dataset\n",
    "\n",
    "check whether while changing datatype we have replaced the special unwanted characters with NaN or not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## missing values before datatype change and string manipulation\n",
    "dataset_raw.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for reference for renamed columns\n",
    "df_renamedCols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## missing values after datatype change and string manipulation\n",
    "\n",
    "dataset.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: We have few missing values across the columns, but the number is small enough. Hence, can be imputed or even can be removed. \n",
    "We will impute this before modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Exploratory Data Analysis\n",
    "Let's do some visual data analysis of the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uni-variate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['bedroom', 'bathroom', 'total_floors', 'seaface', 'sight_viewed', 'condition', 'quality_grade',\n",
    "       'furnished', 'prev_sold', 'house_age', 'renovation_yrs', 'city']\n",
    "list(enumerate(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count plot\n",
    "\n",
    "plt.figure(figsize = (30, 50))\n",
    "for i in enumerate(features):\n",
    "    plt.subplot(7, 2,i[0]+1)\n",
    "    sns.countplot(i[1], data = dataset)\n",
    "    plt.xticks(rotation = 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20, 5))\n",
    "sns.countplot(data = dataset, x= 'sold_month');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The most number of house were sold in the month of April-2015 also in June and July 2014. \n",
    "The least were sold in month of May-2015."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi-Variate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation heatmap\n",
    "\n",
    "plt.figure(figsize = (20,15))\n",
    "sns.heatmap(dataset.corr(), annot=True,mask=np.triu(dataset.corr()),cmap ='coolwarm', vmin = -1, vmax= 1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The following correlated items can be removed to avoid multi-collinearity.\n",
    "1. total_area is totally correlated with lot_area, \n",
    "2. floor_area is highly correlated with living_area.\n",
    "3. quality_grade is correlated with furnished.\n",
    "4. renovated_orNot is correlated with renovation_yrs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Bivariate for Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Months vs Price\n",
    "Variation in Price over the period of Months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## At which month price was higher or lesser\n",
    "sns.factorplot(x='sold_month',y='price',data=dataset, size=8, aspect=2 );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: We can see that in the month of Feb-2015, the prices were cheaper, whereas in the month of April. the price goes high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotsizeX=8 \n",
    "plotsizeY=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(plotsizeX, plotsizeY))\n",
    "sns.scatterplot(dataset['living_area'],dataset['price'],hue=dataset['sight_viewed'],palette='Paired',legend='full');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(plotsizeX, plotsizeY))\n",
    "sns.scatterplot(dataset['living_area'],dataset['price'],hue=dataset['condition'],palette='Paired',legend='full');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(plotsizeX, plotsizeY))\n",
    "sns.scatterplot(dataset['living_area'],dataset['price'],hue=dataset['quality_grade'],palette='Paired',legend='full');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(plotsizeX, plotsizeY))\n",
    "sns.scatterplot(dataset['living_area'],dataset['price'],hue=dataset['furnished'],palette='Paired',legend='full');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(data=dataset, x='total_floors',  y='price', hue='seaface', kind='line');  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the property with seafacing front are costlier than the house without any seafacing front. But it also shows that how with increase in number of floors the seaface house cost so costlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(data=dataset, x='total_floors',  y='price', hue='furnished', kind='line');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: This shows that the furnished house has higher cost and as the number of floors increases beyond 3 then the cost of the house shoots up for furnished house."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import missingno as msn\n",
    "import folium\n",
    "from folium import plugins\n",
    "import branca.colormap as cm\n",
    "\n",
    "\n",
    "m = folium.Map([47 ,-122], zoom_start=5,width=\"%100\",height=\"%100\")\n",
    "locations = list(zip(dataset.latitude, dataset.longitude))\n",
    "cluster = plugins.MarkerCluster(locations=locations,popups=dataset[\"price\"].tolist())\n",
    "m.add_child(cluster)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Processing\n",
    "Data processing before Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_col = ['total_floors', 'seaface', 'longitude', 'total_area']\n",
    " \n",
    "# treating misiing values using median to impute the missing values\n",
    "for i in missing_col:\n",
    "    dataset.loc[dataset.loc[:,i].isnull(),i]=dataset.loc[:,i].median()\n",
    "\n",
    "print(\"count of NULL values after imputation\\n\")\n",
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Linear Regression Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=dataset\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for basic modeling we are dropping city\n",
    "df=df.drop('city', axis=1)\n",
    "\n",
    "# changing data type of zipcode from int to object\n",
    "df=df.astype({'zipcode':'object'})\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## outlier treatment\n",
    "\n",
    "def remove_outlier(col):\n",
    "    sorted(col)\n",
    "    Q1,Q3=np.percentile(col,[25,75])\n",
    "    IQR=Q3-Q1\n",
    "    lower_range= Q1-(1.5 * IQR)\n",
    "    upper_range= Q3+(1.5 * IQR)\n",
    "    return lower_range, upper_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df=df.drop('zipcode', axis=1).columns\n",
    "\n",
    "for i in out_df:\n",
    "    lr = df[i].quantile(0.25)\n",
    "    ur = df[i].quantile(0.75)\n",
    "    df[i] = np.where(df[i] <lr, lr,df[i])\n",
    "    df[i] = np.where(df[i] >ur, ur,df[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(df.corr(), cmap=\"RdBu\")\n",
    "plt.title(\"Correlations Between Variables\", size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_num_cols = list(df.corr()[\"price\"][(df.corr()[\"price\"]>0.1) | (df.corr()[\"price\"]<-0.1)].index)\n",
    "cat_cols = ['zipcode']\n",
    "important_cols = important_num_cols + cat_cols\n",
    "\n",
    "df2 = df[important_cols]\n",
    "df2.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing data type of zipcode from obj to int64\n",
    "df2=df2.astype({'zipcode':'int64'})\n",
    "df2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#categorical coding of zipcode\n",
    "\n",
    "#df['zipcode'] = df_phase1['zipcode'].astype(\"category\").cat.codes\n",
    "#df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## normalising the price distribution (log price)\n",
    "#df2['price_log'] = np.log(df2.price+1)\n",
    "#plt.figure(figsize=(7,5))\n",
    "#sns.distplot(df2['price_log'], fit=norm)\n",
    "#plt.title(\"Log-Price Distribution Plot\",size=15, weight='bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df3=df2.drop('price', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### X, y Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = df3.drop(\"price_log\", axis=1)\n",
    "#y = df3[\"price_log\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df2.drop(\"price\", axis=1)\n",
    "y = df2[\"price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## standarizing the data\n",
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## scaling the data\n",
    "#important_num_cols=['bedroom', 'living_area', 'total_floors', 'quality_grade', 'floor_area',\n",
    "       'basement_area', 'latitude', 'living_area_2015', 'total_area', 'city',\n",
    "       'population', 'population_density', 'bathroom', 'basement_orNot','zipcode']\n",
    "\n",
    "#scaler = StandardScaler()\n",
    "#X[important_num_cols] = scaler.fit_transform(X[important_num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train-Test Split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression using statsmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate X and y into a single dataframe\n",
    "data_train = pd.concat([X_train, y_train], axis=1)\n",
    "data_test=pd.concat([X_test,y_test],axis=1)\n",
    "data_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expr= 'price ~ bedroom + living_area + total_floors + quality_grade + floor_area + basement_area + latitude + living_area_2015 + total_area  + population + population_density + bathroom + basement_orNot + zipcode'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "lm1 = smf.ols(formula= expr, data = data_train).fit()\n",
    "lm1.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lm1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MSE\n",
    "mse = np.mean((lm1.predict(data_train.drop('price',axis=1))-data_train['price'])**2)\n",
    "\n",
    "#Root Mean Squared Error - RMSE\n",
    "np.sqrt(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on Test data\n",
    "y_pred = lm1.predict(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.scatter(y_test['price'], y_pred)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(y_test,\"blue\")\n",
    "plt.plot(y_pred,\"red\")\n",
    "plt.title(\"Actual Vs Predicted Price\")\n",
    "plt.xlabel (\"Data points\")\n",
    "plt.ylabel (\"Predicted Price\");\n",
    "plt.grid(True, color =\"k\")\n",
    "plt.style.use(\"fivethirtyeight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in np.array(lm1.params.reset_index()):\n",
    "    print('({}) * {} +'.format(round(j,2),i),end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df=df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df['predicted_price']= (-13035181.5) + (3758.52) * df.bedroom + (74.83) * df.living_area + (-1484.73) * df.total_floors + (51286.05) * df.quality_grade + (44.3) * df.floor_area + (3.95) * df.basement_area + (592247.9) * df.latitude + (61.13) * df.living_area_2015 + (0.75) * df.total_area + (-2.66) * df.population + (37.17) * df.population_density + (-2866.85) * df.bathroom + (19175.76) * df.basement_orNot + (-156.7) * df.zipcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df['%diff']=(pred_df.predicted_price - pred_df.price)*100/pred_df.price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df[['price','predicted_price', '%diff']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### --------- lets move on to advance modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating datasets for two phase modelling \n",
    "#### (Phase.1 without any treatment to outliers and multi-collinarity and Phase.2 with all th treatment to outliers and multicollinearity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_phase1=dataset ## (modeling without dealing with multicollinearity or outliers )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_phase2=dataset ## (modeling with less features without signs of multi-collinearity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase.1 Modeling without treatment of outliers or multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#categorical coding of city\n",
    "df_phase1['city'] = df_phase1['city'].astype(\"category\").cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_phase1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X, y Split\n",
    "Splitting the data into X and y chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_phase1.drop(\"price\", axis=1)\n",
    "y = df_phase1[\"price\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split\n",
    "Splitting the data into Train and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## splitting the data into 80:20\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining several evaluation functions for convenience\n",
    "\n",
    "def rmse_cv(model):\n",
    "    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=5)).mean()\n",
    "    return rmse\n",
    "    \n",
    "\n",
    "def evaluation(y, predictions):\n",
    "    mae = mean_absolute_error(y, predictions)\n",
    "    mse = mean_squared_error(y, predictions)\n",
    "    rmse = np.sqrt(mean_squared_error(y, predictions))\n",
    "    r_squared = r2_score(y, predictions)\n",
    "    return mae, mse, rmse, r_squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = pd.DataFrame(columns=[\"Model\",\"MAE\",\"MSE\",\"RMSE\",\"R2 Score\",\"RMSE (Cross-Validation)\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model.1 Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)\n",
    "predictions = lin_reg.predict(X_test)\n",
    "\n",
    "mae, mse, rmse, r_squared = evaluation(y_test, predictions)\n",
    "print(\"MAE:\", mae)\n",
    "print(\"MSE:\", mse)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"R2 Score:\", r_squared)\n",
    "print(\"-\"*30)\n",
    "rmse_cross_val = rmse_cv(lin_reg)\n",
    "print(\"RMSE Cross-Validation:\", rmse_cross_val)\n",
    "\n",
    "new_row = {\"Model\": \"LinearRegression\",\"MAE\": mae, \"MSE\": mse, \"RMSE\": rmse, \"R2 Score\": r_squared, \"RMSE (Cross-Validation)\": rmse_cross_val}\n",
    "models = models.append(new_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model.2 Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Ridge()\n",
    "ridge.fit(X_train, y_train)\n",
    "predictions = ridge.predict(X_test)\n",
    "\n",
    "mae, mse, rmse, r_squared = evaluation(y_test, predictions)\n",
    "print(\"MAE:\", mae)\n",
    "print(\"MSE:\", mse)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"R2 Score:\", r_squared)\n",
    "print(\"-\"*30)\n",
    "rmse_cross_val = rmse_cv(ridge)\n",
    "print(\"RMSE Cross-Validation:\", rmse_cross_val)\n",
    "\n",
    "new_row = {\"Model\": \"Ridge\",\"MAE\": mae, \"MSE\": mse, \"RMSE\": rmse, \"R2 Score\": r_squared, \"RMSE (Cross-Validation)\": rmse_cross_val}\n",
    "models = models.append(new_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model.3 Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso()\n",
    "lasso.fit(X_train, y_train)\n",
    "predictions = lasso.predict(X_test)\n",
    "\n",
    "mae, mse, rmse, r_squared = evaluation(y_test, predictions)\n",
    "print(\"MAE:\", mae)\n",
    "print(\"MSE:\", mse)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"R2 Score:\", r_squared)\n",
    "print(\"-\"*30)\n",
    "rmse_cross_val = rmse_cv(lasso)\n",
    "print(\"RMSE Cross-Validation:\", rmse_cross_val)\n",
    "\n",
    "new_row = {\"Model\": \"Lasso\",\"MAE\": mae, \"MSE\": mse, \"RMSE\": rmse, \"R2 Score\": r_squared, \"RMSE (Cross-Validation)\": rmse_cross_val}\n",
    "models = models.append(new_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model.3 Elastic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic_net = ElasticNet()\n",
    "elastic_net.fit(X_train, y_train)\n",
    "predictions = elastic_net.predict(X_test)\n",
    "\n",
    "mae, mse, rmse, r_squared = evaluation(y_test, predictions)\n",
    "print(\"MAE:\", mae)\n",
    "print(\"MSE:\", mse)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"R2 Score:\", r_squared)\n",
    "print(\"-\"*30)\n",
    "rmse_cross_val = rmse_cv(elastic_net)\n",
    "print(\"RMSE Cross-Validation:\", rmse_cross_val)\n",
    "\n",
    "new_row = {\"Model\": \"ElasticNet\",\"MAE\": mae, \"MSE\": mse, \"RMSE\": rmse, \"R2 Score\": r_squared, \"RMSE (Cross-Validation)\": rmse_cross_val}\n",
    "models = models.append(new_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model.4 Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr = SVR(C=100000)\n",
    "svr.fit(X_train, y_train)\n",
    "predictions = svr.predict(X_test)\n",
    "\n",
    "mae, mse, rmse, r_squared = evaluation(y_test, predictions)\n",
    "print(\"MAE:\", mae)\n",
    "print(\"MSE:\", mse)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"R2 Score:\", r_squared)\n",
    "print(\"-\"*30)\n",
    "rmse_cross_val = rmse_cv(svr)\n",
    "print(\"RMSE Cross-Validation:\", rmse_cross_val)\n",
    "\n",
    "new_row = {\"Model\": \"SVR\",\"MAE\": mae, \"MSE\": mse, \"RMSE\": rmse, \"R2 Score\": r_squared, \"RMSE (Cross-Validation)\": rmse_cross_val}\n",
    "models = models.append(new_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model.5 Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest = RandomForestRegressor(n_estimators=100)\n",
    "random_forest.fit(X_train, y_train)\n",
    "predictions = random_forest.predict(X_test)\n",
    "\n",
    "mae, mse, rmse, r_squared = evaluation(y_test, predictions)\n",
    "print(\"MAE:\", mae)\n",
    "print(\"MSE:\", mse)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"R2 Score:\", r_squared)\n",
    "print(\"-\"*30)\n",
    "rmse_cross_val = rmse_cv(random_forest)\n",
    "print(\"RMSE Cross-Validation:\", rmse_cross_val)\n",
    "\n",
    "new_row = {\"Model\": \"RandomForestRegressor\",\"MAE\": mae, \"MSE\": mse, \"RMSE\": rmse, \"R2 Score\": r_squared, \"RMSE (Cross-Validation)\": rmse_cross_val}\n",
    "models = models.append(new_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mode.6 XGBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBRegressor(n_estimators=1000, learning_rate=0.01)\n",
    "xgb.fit(X_train, y_train)\n",
    "predictions = xgb.predict(X_test)\n",
    "\n",
    "mae, mse, rmse, r_squared = evaluation(y_test, predictions)\n",
    "print(\"MAE:\", mae)\n",
    "print(\"MSE:\", mse)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"R2 Score:\", r_squared)\n",
    "print(\"-\"*30)\n",
    "rmse_cross_val = rmse_cv(xgb)\n",
    "print(\"RMSE Cross-Validation:\", rmse_cross_val)\n",
    "\n",
    "new_row = {\"Model\": \"XGBRegressor\",\"MAE\": mae, \"MSE\": mse, \"RMSE\": rmse, \"R2 Score\": r_squared, \"RMSE (Cross-Validation)\": rmse_cross_val}\n",
    "models = models.append(new_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model.7 Polynomial Regression (Degree=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_reg = PolynomialFeatures(degree=2)\n",
    "X_train_2d = poly_reg.fit_transform(X_train)\n",
    "X_test_2d = poly_reg.transform(X_test)\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train_2d, y_train)\n",
    "predictions = lin_reg.predict(X_test_2d)\n",
    "\n",
    "mae, mse, rmse, r_squared = evaluation(y_test, predictions)\n",
    "print(\"MAE:\", mae)\n",
    "print(\"MSE:\", mse)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"R2 Score:\", r_squared)\n",
    "print(\"-\"*30)\n",
    "rmse_cross_val = rmse_cv(lin_reg)\n",
    "print(\"RMSE Cross-Validation:\", rmse_cross_val)\n",
    "\n",
    "new_row = {\"Model\": \"Polynomial Regression (degree=2)\",\"MAE\": mae, \"MSE\": mse, \"RMSE\": rmse, \"R2 Score\": r_squared, \"RMSE (Cross-Validation)\": rmse_cross_val}\n",
    "models = models.append(new_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Comparison\n",
    "1. Mean Absolute Error (MAE) shows the difference between predictions and actual values.\n",
    "2. Mean squared error (MSE) tells you how close a regression line is to a set of points\n",
    "3. Root Mean Square Error (RMSE) shows how accurately the model predicts the response. \n",
    "4. R^2 will be calculated to find the goodness of fit measure.\n",
    "5. RMSE cross validation: The less the Root Mean Squared Error (RMSE), The better the model is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.sort_values(by=\"RMSE (Cross-Validation)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "sns.barplot(x=models[\"Model\"], y=models[\"RMSE (Cross-Validation)\"])\n",
    "plt.title(\"Models' RMSE Scores (Cross-Validated)\", size=15)\n",
    "plt.xticks(rotation=90, size=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we can see that XGB Regressor gives us the best model among all.\n",
    "\n",
    "#### ------------- Moving on to Phase 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase.2 Modeling after treating the outliers and multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Data Preparation before Modeling*\n",
    "### 1. Outliers Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_phase2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## let's boxplot all the numerical columns and see if there any outliers\n",
    "\n",
    "data_plot=df_phase2[['price', 'bedroom', 'living_area', 'lot_area', 'total_floors',\n",
    "       'seaface', 'sight_viewed', 'condition', 'quality_grade', 'floor_area',\n",
    "       'basement_area', 'zipcode', 'latitude', 'longitude', 'living_area_2015',\n",
    "       'lot_area_2015', 'furnished', 'total_area', 'population',\n",
    "       'population_density', 'prev_sold', 'bathroom', 'house_age',\n",
    "       'renovation_yrs', 'renovated_orNot', 'sold_month', 'basement_orNot']]\n",
    "\n",
    "fig=plt.figure(figsize=(20,15));\n",
    "for i in range(0,len(data_plot.columns)):\n",
    "    ax=fig.add_subplot(7,4,i+1)\n",
    "    sns.boxplot(data_plot[data_plot.columns[i]])\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: We can see that we can see the outliers that might affect our results. \n",
    "The variables in which outliers might affect our model are the following: \n",
    "1. price\n",
    "2. bedroom\n",
    "3. bathroom\n",
    "4. living area\n",
    "5. lot area\n",
    "6. floor area\n",
    "7. basement_area\n",
    "8. total area\n",
    "9. living area 2015\n",
    "10. lot area 2015\n",
    "11. total area\n",
    "\n",
    "Here, we can refer the correlation between these and treat outliers based on that. so that we wont have a significant data loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trimming the extreme ouliers without affecting any significant loss of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## bedroom distribution\n",
    "print(\"Skewness\", df_phase2.bedroom.skew())\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.distplot(df_phase2.bedroom)\n",
    "df_phase2.bedroom.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## trimming extreme outliers where number of bedroom is more than 10\n",
    "df_phase2=df_phase2[(df_phase2['bedroom']<=10)]\n",
    "print('loss of data is', (1-(df_phase2.index.size/dataset.index.size))*100, 'percent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## living area distribution\n",
    "print(\"Skewness\", df_phase2.living_area.skew())\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.distplot(df_phase2.living_area)\n",
    "df_phase2.living_area.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## trimming extreme outliers\n",
    "df_phase2=df_phase2[(df_phase2['living_area']<=8000)]\n",
    "print('loss of data is', (1-(df_phase2.index.size/dataset.index.size))*100, 'percent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## bathroom distribution\n",
    "print(\"Skewness\", df_phase2.bathroom.skew())\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.distplot(df_phase2.bathroom)\n",
    "df_phase2.bathroom.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## trimming extreme outliers\n",
    "df_phase2=df_phase2[(df_phase2['bathroom']<=30)]\n",
    "print('loss of data is', (1-(df_phase2.index.size/dataset.index.size))*100, 'percent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## price distribution\n",
    "print(\"Skewness\", df_phase2.price.skew())\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.distplot(df_phase2.price)\n",
    "df_phase2.price.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## trimming extreme outliers\n",
    "df_phase2=df_phase2[(df_phase2['price']<=4000000)]\n",
    "print('loss of data is', (1-(df_phase2.index.size/dataset.index.size))*100, 'percent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## After trimming lets check the boxplot\n",
    "\n",
    "## let's boxplot all the numerical columns and see if there any outliers\n",
    "\n",
    "data_plot=df_phase2[['price', 'bedroom', 'living_area', 'lot_area', 'total_floors',\n",
    "       'seaface', 'sight_viewed', 'condition', 'quality_grade', 'floor_area',\n",
    "       'basement_area', 'zipcode', 'latitude', 'longitude', 'living_area_2015',\n",
    "       'lot_area_2015', 'furnished', 'total_area', 'population',\n",
    "       'population_density', 'prev_sold', 'bathroom', 'house_age',\n",
    "       'renovation_yrs', 'renovated_orNot', 'sold_month', 'basement_orNot']]\n",
    "\n",
    "fig=plt.figure(figsize=(20,15));\n",
    "for i in range(0,len(data_plot.columns)):\n",
    "    ax=fig.add_subplot(7,4,i+1)\n",
    "    sns.boxplot(data_plot[data_plot.columns[i]])\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Only the main variables from which we wish to treat outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Highly correlation varibales\n",
    "1. total_area is totally correlated with lot_area with correlation of 1.\n",
    "2. floor_area is highly correlated with living_area.\n",
    "3. quality_grade is correlated with furnished.\n",
    "4. renovated_orNot is correlated with renovation_yrs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## drop total area which completely correlated variable\n",
    "df_phase2.drop('total_area', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_treated = df_phase2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_data=df_treated[['bedroom','bathroom', 'living_area', 'lot_area', 'floor_area', 'basement_area', 'longitude','living_area_2015','lot_area_2015']]\n",
    "\n",
    "for i in out_data:\n",
    "    lr = df_treated[i].quantile(0.25)\n",
    "    ur = df_treated[i].quantile(0.75)\n",
    "    df_treated[i] = np.where(df_treated[i] <lr, lr,df_treated[i])\n",
    "    df_treated[i] = np.where(df_treated[i] >ur, ur,df_treated[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_plot=df_treated[['bedroom', 'living_area', 'lot_area', 'total_floors',\n",
    "       'seaface', 'sight_viewed', 'condition', 'quality_grade', 'floor_area',\n",
    "       'basement_area', 'zipcode', 'latitude', 'longitude', 'living_area_2015',\n",
    "       'lot_area_2015', 'furnished', 'population',\n",
    "       'population_density', 'prev_sold', 'bathroom', 'house_age',\n",
    "       'renovation_yrs', 'renovated_orNot', 'sold_month', 'basement_orNot']]\n",
    "fig=plt.figure(figsize=(20,15))\n",
    "for i in range(0,len(out_plot.columns)):\n",
    "    ax=fig.add_subplot(7,4,i+1)\n",
    "    sns.boxplot(out_plot[out_plot.columns[i]], whis=3)\n",
    "    plt.tight_layout()\n",
    "print('Shape after Outliers Treatment',df_treated.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## signs of data loss after outlier treatment\n",
    "print('loss of data is', (1-(df_treated.index.size/dataset.index.size))*100, 'percent.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_treated.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standadization of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalising the price distribution (if required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## checking price distribution\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.distplot(df_treated['price'], fit=norm)\n",
    "plt.title(\"Price Distribution Plot\",size=15, weight='bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## price distribution\n",
    "print(\"Skewness\", df_treated.price.skew())\n",
    "df_treated.price.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The above distribution graph shows that there is a right-skewed distribution on price. This means there is a positive skewness. Log transformation will be used to make this feature less skewed. This will help to make easier interpretation and better statistical analysis\n",
    "\n",
    "Since division by zero is a problem, log+1 transformation would be better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_treated['price_log'] = np.log(df_treated.price+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "sns.distplot(df_treated['price_log'], fit=norm)\n",
    "plt.title(\"Log-Price Distribution Plot\",size=15, weight='bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_treated.plot.scatter(x='price', y='price_log', figsize=(7,5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with Multi-Collinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## features which are highly correlated with price of house (Top 15 features except price)\n",
    "df_treated.corr()[\"price\"].nlargest(17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the best features that highly correlated with the price \n",
    "    1. quality_grade         \n",
    "    2. living_area           \n",
    "    3. furnished             \n",
    "    4. living_area_2015      \n",
    "    5. floor_area            \n",
    "    6. bathroom              \n",
    "    7. latitude              \n",
    "    8. sight_viewed          \n",
    "    9. bedroom               \n",
    "    10. total_floors          \n",
    "    11. basement_area         \n",
    "    12. basement_orNot        \n",
    "    13. population_density    \n",
    "    14. seaface               \n",
    "    15. renovated_orNot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking collinearity using correlation map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dython import nominal\n",
    "nominal.associations(df_treated,figsize=(30,20),mark_columns=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zipcode & city is changed from object datatype to int\n",
    "#dataset2 = dataset2.astype({'zipcode':'int64', 'city':'int64', 'sold_month':'int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation heatmap to understand the relation between the variables\n",
    "\n",
    "plt.figure(figsize = (20,15))\n",
    "sns.heatmap(df_treated.corr(), annot=True,mask=np.triu(df_treated.corr()),cmap ='coolwarm', vmin = -1, vmax= 1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## checking multi-collinearity using eigen values\n",
    "\n",
    "corr=df_treated.corr(method='pearson')\n",
    "\n",
    "#Eigen vector of a correlation matrix.\n",
    "multicollinearity, V=np.linalg.eig(corr)\n",
    "multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-collinearity check using Variance Inflation Factor\n",
    "There might be redundant variables in the dataset, to eliminate which we should use VIF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_treated.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Selected_features = df_treated[['price','bedroom', 'living_area', 'lot_area', 'total_floors',\n",
    "       'seaface', 'sight_viewed', 'condition', 'quality_grade', 'floor_area',\n",
    "       'basement_area', 'zipcode', 'latitude', 'longitude', 'living_area_2015',\n",
    "       'lot_area_2015', 'furnished', 'city', 'population',\n",
    "       'population_density', 'prev_sold', 'bathroom', 'house_age',\n",
    "       'renovation_yrs', 'renovated_orNot', 'sold_month', 'basement_orNot',\n",
    "       ]]\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "def calc_vif(X):\n",
    "\n",
    "    # Calculating VIF\n",
    "    vif = pd.DataFrame()\n",
    "    vif[\"variables\"] = X.columns\n",
    "    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "    return(vif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Selected_features.drop('price', axis = 1)\n",
    "calc_vif(X).sort_values(by = 'VIF', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop('zipcode', axis = 1)\n",
    "calc_vif(X).sort_values(by = 'VIF', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop('latitude', axis = 1)\n",
    "calc_vif(X).sort_values(by = 'VIF', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop('longitude', axis = 1)\n",
    "calc_vif(X).sort_values(by = 'VIF', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop('living_area', axis = 1)\n",
    "calc_vif(X).sort_values(by = 'VIF', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop('quality_grade', axis = 1)\n",
    "calc_vif(X).sort_values(by = 'VIF', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop('bedroom', axis = 1)\n",
    "calc_vif(X).sort_values(by = 'VIF', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop('lot_area_2015', axis = 1)\n",
    "calc_vif(X).sort_values(by = 'VIF', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop('floor_area', axis = 1)\n",
    "calc_vif(X).sort_values(by = 'VIF', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop('living_area_2015', axis = 1)\n",
    "calc_vif(X).sort_values(by = 'VIF', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop('condition', axis = 1)\n",
    "calc_vif(X).sort_values(by = 'VIF', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop('bathroom', axis = 1)\n",
    "calc_vif(X).sort_values(by = 'VIF', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop('basement_orNot', axis = 1)\n",
    "calc_vif(X).sort_values(by = 'VIF', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop('lot_area', axis = 1)\n",
    "calc_vif(X).sort_values(by = 'VIF', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop('total_floors', axis = 1)\n",
    "calc_vif(X).sort_values(by = 'VIF', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop('population', axis = 1)\n",
    "calc_vif(X).sort_values(by = 'VIF', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop('city', axis = 1)\n",
    "calc_vif(X).sort_values(by = 'VIF', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using VIF we came with best factors which are not affected by multi-collinearity.\n",
    "\n",
    "1. house_age\n",
    "2. population_density\n",
    "3. sold_month\n",
    "4. renovated_orNot\n",
    "5. renovation_yrs\n",
    "6. basement_area\n",
    "7. sight_viewed\n",
    "8. furnished\n",
    "9. seaface\n",
    "10. prev_sold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building based on Feature selection using VIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_VIF = df_treated[['price','house_age', 'population_density', 'sold_month', 'renovated_orNot','renovation_yrs','basement_area','sight_viewed','furnished','seaface','prev_sold']]\n",
    "df_VIF.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X, y Split\n",
    "Splitting the data into X and y chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_VIF.drop(\"price\", axis=1)\n",
    "y = df_VIF[\"price\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardizing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X[X.columns] = scaler.fit_transform(X[X.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check the standarization\n",
    "X.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_VIF = pd.DataFrame(columns=[\"Model\",\"MAE\",\"MSE\",\"RMSE\",\"R2 Score\",\"RMSE (Cross-Validation)\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)\n",
    "predictions = lin_reg.predict(X_test)\n",
    "\n",
    "mae, mse, rmse, r_squared = evaluation(y_test, predictions)\n",
    "print(\"MAE:\", mae)\n",
    "print(\"MSE:\", mse)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"R2 Score:\", r_squared)\n",
    "print(\"-\"*30)\n",
    "rmse_cross_val = rmse_cv(lin_reg)\n",
    "print(\"RMSE Cross-Validation:\", rmse_cross_val)\n",
    "\n",
    "new_row = {\"Model\": \"LinearRegression\",\"MAE\": mae, \"MSE\": mse, \"RMSE\": rmse, \"R2 Score\": r_squared, \"RMSE (Cross-Validation)\": rmse_cross_val}\n",
    "models_VIF = models_VIF.append(new_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Ridge()\n",
    "ridge.fit(X_train, y_train)\n",
    "predictions = ridge.predict(X_test)\n",
    "\n",
    "mae, mse, rmse, r_squared = evaluation(y_test, predictions)\n",
    "print(\"MAE:\", mae)\n",
    "print(\"MSE:\", mse)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"R2 Score:\", r_squared)\n",
    "print(\"-\"*30)\n",
    "rmse_cross_val = rmse_cv(ridge)\n",
    "print(\"RMSE Cross-Validation:\", rmse_cross_val)\n",
    "\n",
    "new_row = {\"Model\": \"Ridge\",\"MAE\": mae, \"MSE\": mse, \"RMSE\": rmse, \"R2 Score\": r_squared, \"RMSE (Cross-Validation)\": rmse_cross_val}\n",
    "models_VIF = models_VIF.append(new_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso()\n",
    "lasso.fit(X_train, y_train)\n",
    "predictions = lasso.predict(X_test)\n",
    "\n",
    "mae, mse, rmse, r_squared = evaluation(y_test, predictions)\n",
    "print(\"MAE:\", mae)\n",
    "print(\"MSE:\", mse)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"R2 Score:\", r_squared)\n",
    "print(\"-\"*30)\n",
    "rmse_cross_val = rmse_cv(lasso)\n",
    "print(\"RMSE Cross-Validation:\", rmse_cross_val)\n",
    "\n",
    "new_row = {\"Model\": \"Lasso\",\"MAE\": mae, \"MSE\": mse, \"RMSE\": rmse, \"R2 Score\": r_squared, \"RMSE (Cross-Validation)\": rmse_cross_val}\n",
    "models_VIF = models_VIF.append(new_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic_net = ElasticNet()\n",
    "elastic_net.fit(X_train, y_train)\n",
    "predictions = elastic_net.predict(X_test)\n",
    "\n",
    "mae, mse, rmse, r_squared = evaluation(y_test, predictions)\n",
    "print(\"MAE:\", mae)\n",
    "print(\"MSE:\", mse)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"R2 Score:\", r_squared)\n",
    "print(\"-\"*30)\n",
    "rmse_cross_val = rmse_cv(elastic_net)\n",
    "print(\"RMSE Cross-Validation:\", rmse_cross_val)\n",
    "\n",
    "new_row = {\"Model\": \"ElasticNet\",\"MAE\": mae, \"MSE\": mse, \"RMSE\": rmse, \"R2 Score\": r_squared, \"RMSE (Cross-Validation)\": rmse_cross_val}\n",
    "models_VIF = models_VIF.append(new_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr = SVR(C=100000)\n",
    "svr.fit(X_train, y_train)\n",
    "predictions = svr.predict(X_test)\n",
    "\n",
    "mae, mse, rmse, r_squared = evaluation(y_test, predictions)\n",
    "print(\"MAE:\", mae)\n",
    "print(\"MSE:\", mse)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"R2 Score:\", r_squared)\n",
    "print(\"-\"*30)\n",
    "rmse_cross_val = rmse_cv(svr)\n",
    "print(\"RMSE Cross-Validation:\", rmse_cross_val)\n",
    "\n",
    "new_row = {\"Model\": \"SVR\",\"MAE\": mae, \"MSE\": mse, \"RMSE\": rmse, \"R2 Score\": r_squared, \"RMSE (Cross-Validation)\": rmse_cross_val}\n",
    "models_VIF = models_VIF.append(new_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest = RandomForestRegressor(n_estimators=100)\n",
    "random_forest.fit(X_train, y_train)\n",
    "predictions = random_forest.predict(X_test)\n",
    "\n",
    "mae, mse, rmse, r_squared = evaluation(y_test, predictions)\n",
    "print(\"MAE:\", mae)\n",
    "print(\"MSE:\", mse)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"R2 Score:\", r_squared)\n",
    "print(\"-\"*30)\n",
    "rmse_cross_val = rmse_cv(random_forest)\n",
    "print(\"RMSE Cross-Validation:\", rmse_cross_val)\n",
    "\n",
    "new_row = {\"Model\": \"RandomForestRegressor\",\"MAE\": mae, \"MSE\": mse, \"RMSE\": rmse, \"R2 Score\": r_squared, \"RMSE (Cross-Validation)\": rmse_cross_val}\n",
    "models_VIF = models_VIF.append(new_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBRegressor(n_estimators=1000, learning_rate=0.01)\n",
    "xgb.fit(X_train, y_train)\n",
    "predictions = xgb.predict(X_test)\n",
    "\n",
    "mae, mse, rmse, r_squared = evaluation(y_test, predictions)\n",
    "print(\"MAE:\", mae)\n",
    "print(\"MSE:\", mse)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"R2 Score:\", r_squared)\n",
    "print(\"-\"*30)\n",
    "rmse_cross_val = rmse_cv(xgb)\n",
    "print(\"RMSE Cross-Validation:\", rmse_cross_val)\n",
    "\n",
    "new_row = {\"Model\": \"XGBRegressor\",\"MAE\": mae, \"MSE\": mse, \"RMSE\": rmse, \"R2 Score\": r_squared, \"RMSE (Cross-Validation)\": rmse_cross_val}\n",
    "models_VIF = models_VIF.append(new_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Polynomial Regression (Degree=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_reg = PolynomialFeatures(degree=2)\n",
    "X_train_2d = poly_reg.fit_transform(X_train)\n",
    "X_test_2d = poly_reg.transform(X_test)\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train_2d, y_train)\n",
    "predictions = lin_reg.predict(X_test_2d)\n",
    "\n",
    "mae, mse, rmse, r_squared = evaluation(y_test, predictions)\n",
    "print(\"MAE:\", mae)\n",
    "print(\"MSE:\", mse)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"R2 Score:\", r_squared)\n",
    "print(\"-\"*30)\n",
    "rmse_cross_val = rmse_cv(lin_reg)\n",
    "print(\"RMSE Cross-Validation:\", rmse_cross_val)\n",
    "\n",
    "new_row = {\"Model\": \"Polynomial Regression (degree=2)\",\"MAE\": mae, \"MSE\": mse, \"RMSE\": rmse, \"R2 Score\": r_squared, \"RMSE (Cross-Validation)\": rmse_cross_val}\n",
    "models_VIF = models_VIF.append(new_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Comparison_VIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_VIF.sort_values(by=\"RMSE (Cross-Validation)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "sns.barplot(x=models_VIF[\"Model\"], y=models_VIF[\"RMSE (Cross-Validation)\"])\n",
    "plt.title(\"Models' RMSE Scores (Cross-Validated)\", size=15)\n",
    "plt.xticks(rotation=30, size=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling with one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_phase3 = df_treated\n",
    "df_phase3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_phase3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_phase3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p3= df_phase3.drop('price_log', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting dummies for columns ceil, coast, sight, condition, quality, yr_renovated, furnished\n",
    "df_ohc = pd.get_dummies(df_p3, columns=['zipcode','city','sold_month'],drop_first=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ohc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ohc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating X, y for training and testing set\n",
    "X = df_ohc.drop(\"price\" , axis=1)\n",
    "y = df_ohc[\"price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_ohc = pd.DataFrame(columns=[\"Model\",\"MAE\",\"MSE\",\"RMSE\",\"R2 Score\",\"RMSE (Cross-Validation)\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)\n",
    "predictions = lin_reg.predict(X_test)\n",
    "\n",
    "mae, mse, rmse, r_squared = evaluation(y_test, predictions)\n",
    "print(\"MAE:\", mae)\n",
    "print(\"MSE:\", mse)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"R2 Score:\", r_squared)\n",
    "print(\"-\"*30)\n",
    "rmse_cross_val = rmse_cv(lin_reg)\n",
    "print(\"RMSE Cross-Validation:\", rmse_cross_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_row = {\"Model\": \"LinearRegression\",\"MAE\": mae, \"MSE\": mse, \"RMSE\": rmse, \"R2 Score\": r_squared, \"RMSE (Cross-Validation)\": rmse_cross_val}\n",
    "models_ohc = models_ohc.append(new_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Ridge()\n",
    "ridge.fit(X_train, y_train)\n",
    "predictions = ridge.predict(X_test)\n",
    "\n",
    "mae, mse, rmse, r_squared = evaluation(y_test, predictions)\n",
    "print(\"MAE:\", mae)\n",
    "print(\"MSE:\", mse)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"R2 Score:\", r_squared)\n",
    "print(\"-\"*30)\n",
    "rmse_cross_val = rmse_cv(ridge)\n",
    "print(\"RMSE Cross-Validation:\", rmse_cross_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_row = {\"Model\": \"Ridge\",\"MAE\": mae, \"MSE\": mse, \"RMSE\": rmse, \"R2 Score\": r_squared, \"RMSE (Cross-Validation)\": rmse_cross_val}\n",
    "models_VIF = models_VIF.append(new_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso()\n",
    "lasso.fit(X_train, y_train)\n",
    "predictions = lasso.predict(X_test)\n",
    "\n",
    "mae, mse, rmse, r_squared = evaluation(y_test, predictions)\n",
    "print(\"MAE:\", mae)\n",
    "print(\"MSE:\", mse)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"R2 Score:\", r_squared)\n",
    "print(\"-\"*30)\n",
    "rmse_cross_val = rmse_cv(lasso)\n",
    "print(\"RMSE Cross-Validation:\", rmse_cross_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_row = {\"Model\": \"Lasso\",\"MAE\": mae, \"MSE\": mse, \"RMSE\": rmse, \"R2 Score\": r_squared, \"RMSE (Cross-Validation)\": rmse_cross_val}\n",
    "models_ohc = models_ohc.append(new_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic_net = ElasticNet()\n",
    "elastic_net.fit(X_train, y_train)\n",
    "predictions = elastic_net.predict(X_test)\n",
    "\n",
    "mae, mse, rmse, r_squared = evaluation(y_test, predictions)\n",
    "print(\"MAE:\", mae)\n",
    "print(\"MSE:\", mse)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"R2 Score:\", r_squared)\n",
    "print(\"-\"*30)\n",
    "rmse_cross_val = rmse_cv(elastic_net)\n",
    "print(\"RMSE Cross-Validation:\", rmse_cross_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_row = {\"Model\": \"ElasticNet\",\"MAE\": mae, \"MSE\": mse, \"RMSE\": rmse, \"R2 Score\": r_squared, \"RMSE (Cross-Validation)\": rmse_cross_val}\n",
    "models_ohc = models_ohc.append(new_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr = SVR(C=100000)\n",
    "svr.fit(X_train, y_train)\n",
    "predictions = svr.predict(X_test)\n",
    "\n",
    "mae, mse, rmse, r_squared = evaluation(y_test, predictions)\n",
    "print(\"MAE:\", mae)\n",
    "print(\"MSE:\", mse)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"R2 Score:\", r_squared)\n",
    "print(\"-\"*30)\n",
    "rmse_cross_val = rmse_cv(svr)\n",
    "print(\"RMSE Cross-Validation:\", rmse_cross_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_row = {\"Model\": \"SVR\",\"MAE\": mae, \"MSE\": mse, \"RMSE\": rmse, \"R2 Score\": r_squared, \"RMSE (Cross-Validation)\": rmse_cross_val}\n",
    "models_ohc = models_ohc.append(new_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest = RandomForestRegressor(n_estimators=100)\n",
    "random_forest.fit(X_train, y_train)\n",
    "predictions = random_forest.predict(X_test)\n",
    "\n",
    "mae, mse, rmse, r_squared = evaluation(y_test, predictions)\n",
    "print(\"MAE:\", mae)\n",
    "print(\"MSE:\", mse)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"R2 Score:\", r_squared)\n",
    "print(\"-\"*30)\n",
    "rmse_cross_val = rmse_cv(random_forest)\n",
    "print(\"RMSE Cross-Validation:\", rmse_cross_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_row = {\"Model\": \"RandomForestRegressor\",\"MAE\": mae, \"MSE\": mse, \"RMSE\": rmse, \"R2 Score\": r_squared, \"RMSE (Cross-Validation)\": rmse_cross_val}\n",
    "models_ohc = models_ohc.append(new_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBRegressor(n_estimators=1000, learning_rate=0.01)\n",
    "xgb.fit(X_train, y_train)\n",
    "predictions = xgb.predict(X_test)\n",
    "\n",
    "mae, mse, rmse, r_squared = evaluation(y_test, predictions)\n",
    "print(\"MAE:\", mae)\n",
    "print(\"MSE:\", mse)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"R2 Score:\", r_squared)\n",
    "print(\"-\"*30)\n",
    "rmse_cross_val = rmse_cv(xgb)\n",
    "print(\"RMSE Cross-Validation:\", rmse_cross_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_row = {\"Model\": \"XGBRegressor\",\"MAE\": mae, \"MSE\": mse, \"RMSE\": rmse, \"R2 Score\": r_squared, \"RMSE (Cross-Validation)\": rmse_cross_val}\n",
    "models_ohc = models_ohc.append(new_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "knn = KNeighborsRegressor(n_neighbors=4,weights='distance')\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "#predicting result over test data\n",
    "predictions= knn.predict(X_test)\n",
    "\n",
    "mae, mse, rmse, r_squared = evaluation(y_test, predictions)\n",
    "print(\"MAE:\", mae)\n",
    "print(\"MSE:\", mse)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"R2 Score:\", r_squared)\n",
    "print(\"-\"*30)\n",
    "rmse_cross_val = rmse_cv(knn)\n",
    "print(\"RMSE Cross-Validation:\", rmse_cross_val)\n",
    "\n",
    "new_row = {\"Model\": \"KNN_Regressor\",\"MAE\": mae, \"MSE\": mse, \"RMSE\": rmse, \"R2 Score\": r_squared, \"RMSE (Cross-Validation)\": rmse_cross_val}\n",
    "models_ohc = models_ohc.append(new_row, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "DT = DecisionTreeRegressor()\n",
    "DT.fit(X_train, y_train)\n",
    "\n",
    "#predicting result over test data\n",
    "predictions= DT.predict(X_test)\n",
    "\n",
    "mae, mse, rmse, r_squared = evaluation(y_test, predictions)\n",
    "print(\"MAE:\", mae)\n",
    "print(\"MSE:\", mse)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"R2 Score:\", r_squared)\n",
    "print(\"-\"*30)\n",
    "rmse_cross_val = rmse_cv(DT)\n",
    "print(\"RMSE Cross-Validation:\", rmse_cross_val)\n",
    "\n",
    "new_row = {\"Model\": \"DecisionTree_Regressor\",\"MAE\": mae, \"MSE\": mse, \"RMSE\": rmse, \"R2 Score\": r_squared, \"RMSE (Cross-Validation)\": rmse_cross_val}\n",
    "models_ohc = models_ohc.append(new_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bagging and Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor, BaggingRegressor\n",
    "GB = GradientBoostingRegressor(n_estimators = 200, learning_rate = 0.1, random_state=22)\n",
    "GB.fit(X_train, y_train)\n",
    "\n",
    "#predicting result over test data\n",
    "predictions= GB.predict(X_test)\n",
    "\n",
    "mae, mse, rmse, r_squared = evaluation(y_test, predictions)\n",
    "print(\"MAE:\", mae)\n",
    "print(\"MSE:\", mse)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"R2 Score:\", r_squared)\n",
    "print(\"-\"*30)\n",
    "rmse_cross_val = rmse_cv(GB)\n",
    "print(\"RMSE Cross-Validation:\", rmse_cross_val)\n",
    "\n",
    "new_row = {\"Model\": \"GradientBoosting_Regressor\",\"MAE\": mae, \"MSE\": mse, \"RMSE\": rmse, \"R2 Score\": r_squared, \"RMSE (Cross-Validation)\": rmse_cross_val}\n",
    "models_ohc = models_ohc.append(new_row, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BGG=BaggingRegressor(n_estimators=50, oob_score= True,random_state=14)\n",
    "BGG.fit(X_train, y_train)\n",
    "\n",
    "#predicting result over test data\n",
    "predictions= BGG.predict(X_test)\n",
    "\n",
    "mae, mse, rmse, r_squared = evaluation(y_test, predictions)\n",
    "print(\"MAE:\", mae)\n",
    "print(\"MSE:\", mse)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"R2 Score:\", r_squared)\n",
    "print(\"-\"*30)\n",
    "rmse_cross_val = rmse_cv(BGG)\n",
    "print(\"RMSE Cross-Validation:\", rmse_cross_val)\n",
    "\n",
    "new_row = {\"Model\": \"BaggingRegressor_Regressor\",\"MAE\": mae, \"MSE\": mse, \"RMSE\": rmse, \"R2 Score\": r_squared, \"RMSE (Cross-Validation)\": rmse_cross_val}\n",
    "models_ohc = models_ohc.append(new_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_ohc.sort_values(by=\"RMSE (Cross-Validation)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "sns.barplot(x=models_ohc[\"Model\"], y=models_VIF[\"RMSE (Cross-Validation)\"])\n",
    "plt.title(\"Models' RMSE Scores (Cross-Validated)\", size=15)\n",
    "plt.xticks(rotation=30, size=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_imp_feature_1=pd.DataFrame(xgb.feature_importances_, columns = [\"Imp\"], index = X_val.columns)\n",
    "rf_imp_feature_1.sort_values(by=\"Imp\",ascending=False)\n",
    "rf_imp_feature_1['Imp'] = rf_imp_feature_1['Imp'].map('{0:.5f}'.format)\n",
    "rf_imp_feature_1=rf_imp_feature_1.sort_values(by=\"Imp\",ascending=False)\n",
    "rf_imp_feature_1.Imp=rf_imp_feature_1.Imp.astype(\"float\")\n",
    "\n",
    "rf_imp_feature_1[:30].plot.bar(figsize=(10,5))\n",
    "\n",
    "#First 20 features have an importance of 90.5% and first 30 have importance of 95.15\n",
    "print(\"First 20 feature importance:\\t\",(rf_imp_feature_1[:20].sum())*100)\n",
    "print(\"First 30 feature importance:\\t\",(rf_imp_feature_1[:30].sum())*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above are top 30 important features that account for 95% of variation in model. \n",
    "We will further analyse for the hypertuning of the models for a better score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering out the important features for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_imp_feature_1[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result (model,pipe_model,X_train_set,y_train_set,X_val_set,y_val_set):\n",
    "    pipe_model.fit(X_train_set,y_train_set)\n",
    "    #predicting result over test data\n",
    "    y_train_predict= pipe_model.predict(X_train_set)\n",
    "    y_val_predict= pipe_model.predict(X_val_set)\n",
    "\n",
    "    trscore=r2_score(y_train_set,y_train_predict)\n",
    "    trRMSE=np.sqrt(mean_squared_error(y_train_set,y_train_predict))\n",
    "    trMSE=mean_squared_error(y_train_set,y_train_predict)\n",
    "    trMAE=mean_absolute_error(y_train_set,y_train_predict)\n",
    "    \n",
    "    vlscore=r2_score(y_val,y_val_predict)\n",
    "    vlRMSE=np.sqrt(mean_squared_error(y_val,y_val_predict))\n",
    "    vlMSE=mean_squared_error(y_val,y_val_predict)\n",
    "    vlMAE=mean_absolute_error(y_val,y_val_predict)\n",
    "    result_df=pd.DataFrame({'Method':[model],'val score':vlscore,'RMSE_val':vlRMSE,'MSE_val':vlMSE,'MSE_vl': vlMSE,\n",
    "                          'train Score':trscore,'RMSE_tr': trRMSE,'MSE_tr': trMSE, 'MAE_tr': trMAE})  \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dff=pd.DataFrame()\n",
    "pipe_LR = Pipeline([('LR', LinearRegression())])\n",
    "result_dff=pd.concat([result_dff,result('LR',pipe_LR,X_train,y_train,X_val,y_val)])\n",
    "\n",
    "pipe_knr = Pipeline([('KNNR', KNeighborsRegressor(n_neighbors=4,weights='distance'))])\n",
    "result_dff=pd.concat([result_dff,result('KNNR',pipe_knr,X_train,y_train,X_val,y_val)])\n",
    "\n",
    "pipe_DTR = Pipeline([('DTR', DecisionTreeRegressor())])\n",
    "result_dff=pd.concat([result_dff,result('DTR',pipe_DTR,X_train,y_train,X_val,y_val)])\n",
    "\n",
    "pipe_GBR = Pipeline([('GBR', GradientBoostingRegressor(n_estimators = 200, learning_rate = 0.1, random_state=22))])\n",
    "result_dff=pd.concat([result_dff,result('GBR',pipe_GBR,X_train,y_train,X_val,y_val)])\n",
    "\n",
    "pipe_BGR = Pipeline([('BGR', BaggingRegressor(n_estimators=50, oob_score= True,random_state=14))])\n",
    "result_dff=pd.concat([result_dff,result('BGR',pipe_BGR,X_train,y_train,X_val,y_val)])\n",
    "\n",
    "pipe_RFR = Pipeline([('RFR', RandomForestRegressor())])\n",
    "result_dff=pd.concat([result_dff,result('RFR',pipe_RFR,X_train,y_train,X_val,y_val)])\n",
    "\n",
    "result_dff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The best model is still from Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling Summary\n",
    "<li>The ensemble models have performed well compared to that of linear,KNN,SVR models\n",
    "<li>The best performance is given by Gradient boosting model with training (score-90%,RMSE- 108130), Validation (score-89.3%,RSME-109201).\n",
    "<li>The top key features that drive the price of the property are: 'furnished','latitude', 'zipcode', 'quality_grade', living_measure','quality_8', 'HouseLandRatio', 'lot_measure15', 'quality_9', 'ceil_measure', 'total_area'.\n",
    "The above data is also reinforced by the analysis done during bivariate analysis.\n",
    "For further improvization, the datasets can be made by treating outliers in different ways and hypertuning the ensemble models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
